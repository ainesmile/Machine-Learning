{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def _cal_d(X, m, E, r):\n",
    "    \n",
    "    \"\"\"\n",
    "    # at (step = k), we have the best feature x_k, alpha_k, and r_k\n",
    "    # then calculate the moving direction for the next step, d_k such that:\n",
    "    \n",
    "        # <X*E_k, X*d_k> = constant_1*S_k,\n",
    "        \n",
    "    # here S_k = (sign(<x_0, r_k>), ..., sign(<x_k, r_k>)), a column vector\n",
    "    # and, constant_1 is any constant that != 0\n",
    "    \n",
    "    # According to (step = k-1), we have:\n",
    "    \n",
    "        # <X*E_k, r_k> = constant_2*S_k,\n",
    "    \n",
    "    # then we got:\n",
    "    \n",
    "        # <X*E_k, X*d_k> = <X*E_k, r_k>\n",
    "    \"\"\"\n",
    "        \n",
    "    if E.shape == (m, ):\n",
    "        E = E[:, np.newaxis]\n",
    "        \n",
    "    Z = X.dot(E)\n",
    "    G = Z.T.dot(Z)\n",
    "    G_inv = np.linalg.inv(G)\n",
    "    _d = E.dot(G_inv)\n",
    "    _d_ = _d.dot(Z.T)\n",
    "    d = _d_.dot(r)\n",
    "    return d\n",
    "\n",
    "def _cal_alpha(X, x, pre_best_x, pre_r, d):\n",
    "    \"\"\"\n",
    "    # alpha need to meet:\n",
    "    \n",
    "    #    C0: |<x_k, r_k>| = |<x_k-1, r_k>|\n",
    "    \n",
    "    # here, r_k = r_k-1 - alpha*X*d_k-1\n",
    "    \n",
    "    # with constraint:\n",
    "    \n",
    "    #    C1: |<x_k, r_k>| < |<x_k-1, r_k-1|\n",
    "    \n",
    "    \n",
    "    # according to:\n",
    "    #    L1: |<x_k, r_k>| = |<x_k-1, r_k>|\n",
    "    # and,\n",
    "    #    L2: <x_k, alpha*X.dot(d)> = <x_k-1, r_k-1>\n",
    "    # and,\n",
    "    #    L3: r_k = r_k-1 - alpha*X*d_k-1\n",
    "    \n",
    "    # constraint C1 equals to:\n",
    "    \n",
    "    #    |<x_k-1, r_k>| = |<x_k-1, r_k-1 - alpha*X*d_k-1>| = \n",
    "    #        |<x_k-1, r_k-1> - alpha*<x_k-1, X*d_k-1>| = \n",
    "    #        |<x_k-1, r_k-1> - alpha*<x_k-1, r_k-1>| < |<x_k-1, r_k-1|\n",
    "\n",
    "    # that constraint C1 requires:\n",
    "    \n",
    "    # |1-alpha| < 1 \n",
    "    \"\"\"\n",
    "    \n",
    "    alpha = None\n",
    "    \n",
    "    X_d = X.dot(d)\n",
    "    \n",
    "    x_pre_r = x.dot(pre_r)\n",
    "    pre_best_x_pre_r = pre_best_x.dot(pre_r)\n",
    "    \n",
    "    x_ = x.dot(X_d)\n",
    "    pre_best_x_ = pre_best_x.dot(X_d)\n",
    "    \n",
    "    alpha_plus = (x_pre_r - pre_best_x_pre_r)/(x_ - pre_best_x_)\n",
    "    alpha_minus = (x_pre_r + pre_best_x_pre_r)/(x_ + pre_best_x_)\n",
    "    \n",
    "    if 0 < alpha_plus < 2:\n",
    "        alpha = alpha_plus\n",
    "    if (0 < alpha_minus < 2) and (alpha_minus < alpha_plus):\n",
    "        alpha = alpha_minus\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "def _cal_beta(alpha_best, beta, pre_d):\n",
    "    beta = beta + alpha_best*pre_d\n",
    "    return beta\n",
    "\n",
    "def _last_alpha(X, r, x, d):\n",
    "    \"\"\"\n",
    "    # <last_x, last_r> = 0\n",
    "    #last_r = r - alpha*X*d\n",
    "    \"\"\"\n",
    "    a = r.dot(x)\n",
    "    delta = X.dot(d)\n",
    "    b = delta.dot(x)\n",
    "    return a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least Angle Regression\n",
    "\n",
    "def lars(X_raw, y_raw):\n",
    "    \"\"\"\n",
    "    References: http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Important: assume all the feature are linearly independent\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1. init\n",
    "    standScaler = StandardScaler()\n",
    "    X = standScaler.fit_transform(X_raw, y_raw)\n",
    "    y = y_raw\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    beta_path = np.zeros(m)\n",
    "    \n",
    "    # F: all feature indices; A: chosen feature indices; L: the left feature indices\n",
    "    F = np.arange(m)\n",
    "    A = []\n",
    "    L = list(set(F) - set(A))\n",
    "    \n",
    "    # X.dot(E) = chosen features ordered as chosen order\n",
    "    E = None\n",
    "    \n",
    "    beta = np.zeros(m)\n",
    "    y_hat = X.dot(beta)\n",
    "    r = y - y_hat\n",
    "    \n",
    "    # 2. step 0\n",
    "    \n",
    "    # 2.1 step 0 results for the first best feature\n",
    "    best_index = np.argmax(np.abs(np.matmul(X.T, r)))\n",
    "    best_x = X[:, best_index]\n",
    "    \n",
    "    # 2.2 prepare for the next step\n",
    "    A.append(best_index)\n",
    "    L = list(set(F) - set(A))\n",
    "    \n",
    "    e = np.zeros(m)\n",
    "    np.put(e, best_index, 1)\n",
    "    E = e\n",
    "    \n",
    "    d = _cal_d(X, m, E, r)\n",
    "    \n",
    "    # 3. start iteration\n",
    "    while L != []:\n",
    "        \n",
    "        # best feature and best alpha for the current step\n",
    "        best_alpha = None\n",
    "        best_index = None\n",
    "        \n",
    "        for i in L:\n",
    "            x = X[:, i]\n",
    "            alpha = _cal_alpha(X, x, best_x, r, d)\n",
    "            if alpha is not None:\n",
    "                if (best_alpha is None) or (best_alpha > alpha):\n",
    "                    best_alpha = alpha\n",
    "                    best_index = i\n",
    "                \n",
    "        best_x = X[:, best_index]\n",
    "        beta = _cal_beta(best_alpha, beta, d)\n",
    "        \n",
    "        beta_path = np.column_stack((beta_path, beta))\n",
    "#         beta_path.append(list(beta))\n",
    "        \n",
    "        r = y - X.dot(beta)\n",
    "        \n",
    "        # prepare for the next step\n",
    "        e = np.zeros(m)\n",
    "        np.put(e, best_index, 1)\n",
    "        E = np.column_stack((E, e))\n",
    "        \n",
    "        d = _cal_d(X, m, E, r)\n",
    "        \n",
    "        A.append(best_index)\n",
    "        L = list(set(F) - set(A))\n",
    "        \n",
    "    # 4. the last step \n",
    "    ## no next best feature, so best_alpha calculate method changes\n",
    "    if L == []:\n",
    "        #last alpha, such that last_x.T.dot(last_r) = 0\n",
    "        last_alpha = _last_alpha(X, r, x, d)\n",
    "        last_beta = _cal_beta(last_alpha, beta, d)\n",
    "        beta = last_beta\n",
    "        \n",
    "        beta_path = np.column_stack((beta_path, beta))\n",
    "#         beta_path.append(list(beta))\n",
    "        \n",
    "    # A is also the feature chosen order\n",
    "    return beta, beta_path, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using sklearn diabetes dataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data, target = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected features ordered as select order are [2, 8, 3, 6, 1, 9, 4, 7, 5, 0]\n",
      "coefs [ -0.47623169 -11.40703082  24.72625713  15.42967916 -37.68035801\n",
      "  22.67648701   4.80620008   8.422084    35.73471316   3.21661161]\n"
     ]
    }
   ],
   "source": [
    "beta, beta_path, A = lars(data, target)\n",
    "print('selected features ordered as select order are', A)\n",
    "print('coefs', beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version lars selected features(ordered) are [2, 8, 3, 6, 1, 9, 4, 7, 5, 0]\n",
      "sklearn version lars coefs [ -0.47623169 -11.40703082  24.72625713  15.42967916 -37.68035801\n",
      "  22.67648701   4.80620008   8.422084    35.73471316   3.21661161]\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Compare to sklearn's Lars to check if it is correct\n",
    "from sklearn.linear_model import Lars\n",
    "\n",
    "# using StandardScaler instead\n",
    "standScaler = StandardScaler()\n",
    "scaled_data = standScaler.fit_transform(data, target)\n",
    "sklearn_lars = Lars(normalize=False)\n",
    "sklearn_lars.fit(scaled_data, target)\n",
    "\n",
    "print('sklearn version lars selected features(ordered) are', sklearn_lars.active_)\n",
    "print('sklearn version lars coefs', sklearn_lars.coef_)\n",
    "\n",
    "print(np.allclose(sklearn_lars.coef_, beta))\n",
    "print(np.allclose(sklearn_lars.active_, A))\n",
    "print(np.allclose(beta_path, sklearn_lars.coef_path_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
